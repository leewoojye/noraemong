# transcribe_and_sync.py

import time
import json
import os
from pathlib import Path
import argparse
from typing import List, Dict, Optional, Tuple
import difflib
import re
from dataclasses import dataclass
import unicodedata

# Audio processing and speech recognition
from faster_whisper import WhisperModel

# Text processing for lyrics alignment
try:
    from fuzzywuzzy import fuzz, process
except ImportError:
    print("⚠️  fuzzywuzzy not installed. Installing...")
    os.system("pip install fuzzywuzzy python-levenshtein")
    from fuzzywuzzy import fuzz, process

@dataclass
class LyricSegment:
    """Represents a synchronized lyric segment with timing information."""
    start_time: float
    end_time: float
    text: str
    confidence: float = 0.0
    word_timings: Optional[List[Dict]] = None

def format_time(seconds: float) -> str:
    """초를 SRT 타임스탬프 형식(HH:MM:SS,ms)으로 변환합니다."""
    milliseconds = round(seconds * 1000.0)
    hours = milliseconds // 3_600_000
    milliseconds %= 3_600_000
    minutes = milliseconds // 60_000
    milliseconds %= 60_000
    seconds = milliseconds // 1_000
    milliseconds %= 1_000
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

def save_as_srt(segments: List[LyricSegment], output_path: Path):
    """추출된 세그먼트를 .srt 자막 파일 형식으로 저장합니다."""
    with open(output_path, 'w', encoding='utf-8') as srt_file:
        for i, segment in enumerate(segments):
            start = format_time(segment.start_time)
            end = format_time(segment.end_time)
            text = segment.text.strip()
            
            srt_file.write(f"{i + 1}\n")
            srt_file.write(f"{start} --> {end}\n")
            srt_file.write(f"{text}\n\n")
    print(f"🎬 SRT 자막 파일 저장 완료: {output_path}")

def save_as_txt(segments: List[LyricSegment], output_path: Path):
    """추출된 텍스트를 .txt 파일 형식으로 저장합니다."""
    with open(output_path, 'w', encoding='utf-8') as txt_file:
        for segment in segments:
            txt_file.write(segment.text.strip() + "\n")
    print(f"📄 일반 텍스트 파일 저장 완료: {output_path}")

def save_as_lrc(segments: List[LyricSegment], output_path: Path):
    """추출된 세그먼트를 .lrc 가사 파일 형식으로 저장합니다."""
    with open(output_path, 'w', encoding='utf-8') as lrc_file:
        lrc_file.write("[ar:Generated by AI Transcription]\n")
        lrc_file.write("[ti:Synchronized Lyrics]\n")
        lrc_file.write("[by:AI Lyrics Sync]\n\n")
        
        for segment in segments:
            minutes = int(segment.start_time // 60)
            seconds = segment.start_time % 60
            timestamp = f"[{minutes:02d}:{seconds:05.2f}]"
            lrc_file.write(f"{timestamp}{segment.text}\n")
    print(f"🎵 LRC 가사 파일 저장 완료: {output_path}")

def save_as_json(segments: List[LyricSegment], output_path: Path):
    """추출된 세그먼트를 상세한 JSON 형식으로 저장합니다."""
    data = {
        "metadata": {
            "generator": "AI Transcription & Sync",
            "version": "1.0",
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_segments": len(segments)
        },
        "segments": []
    }
    
    for segment in segments:
        segment_data = {
            "start_time": segment.start_time,
            "end_time": segment.end_time,
            "duration": segment.end_time - segment.start_time,
            "text": segment.text,
            "confidence": segment.confidence
        }
        
        if segment.word_timings:
            segment_data["word_timings"] = segment.word_timings
        
        data["segments"].append(segment_data)
    
    with open(output_path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, indent=2, ensure_ascii=False)
    print(f"📊 JSON 데이터 파일 저장 완료: {output_path}")

class LyricsProcessor:
    """가사/텍스트 파일 처리 및 정규화를 담당합니다."""
    
    def load_lyrics_file(self, file_path: str) -> List[str]:
        """다양한 형식의 가사 파일을 로드합니다."""
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"가사 파일을 찾을 수 없습니다: {file_path}")
            
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        if file_path.suffix.lower() == '.lrc':
            return self._parse_lrc_text(content)
        elif file_path.suffix.lower() == '.srt':
            return self._parse_srt_text(content)
        else:
            return self._parse_plain_text(content)
    
    def _parse_plain_text(self, content: str) -> List[str]:
        """일반 텍스트 가사를 파싱합니다."""
        lines = content.split('\n')
        lyrics = [line.strip() for line in lines if line.strip()]
        return lyrics
    
    def _parse_lrc_text(self, content: str) -> List[str]:
        """LRC 형식에서 텍스트만 추출합니다."""
        lines = content.split('\n')
        lyrics = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # LRC 타임스탬프 형식 [mm:ss.xx] 제거
            text = re.sub(r'\[\d+:\d+\.\d+\]', '', line).strip()
            if text:
                lyrics.append(text)
                
        return lyrics
    
    def _parse_srt_text(self, content: str) -> List[str]:
        """SRT 형식에서 텍스트만 추출합니다."""
        lines = content.split('\n')
        lyrics = []
        
        for line in lines:
            line = line.strip()
            # 시퀀스 번호, 타임스탬프, 빈 줄 건너뛰기
            if (not line or 
                line.isdigit() or 
                '-->' in line):
                continue
            lyrics.append(line)
            
        return lyrics
    
    def normalize_text(self, text: str) -> str:
        """텍스트 매칭을 위한 정규화를 수행합니다."""
        text = text.lower()
        text = ' '.join(text.split())
        text = re.sub(r'[^\w\s]', ' ', text)
        text = ' '.join(text.split())
        text = unicodedata.normalize('NFKD', text)
        return text

class AudioTranscriber:
    """오디오 전사 및 가사 동기화를 처리합니다."""
    
    def __init__(self, model_size: str = "base", device: str = "auto"):
        self.model_size = model_size
        self.device = device
        self.model = None
        self.lyrics_processor = LyricsProcessor()
        self._load_model()
    
    def _load_model(self):
        """Whisper 모델을 로드합니다."""
        print(f"📥 Whisper 모델 로딩 중: {self.model_size}")
        try:
            self.model = WhisperModel(self.model_size, device=self.device)
            print("✅ Whisper 모델 로드 완료")
        except Exception as e:
            print(f"❌ Whisper 모델 로드 실패: {e}")
            raise
    
    def transcribe_with_timestamps(self, audio_path: str, language: Optional[str] = None) -> List[LyricSegment]:
        """오디오를 전사하고 타임스탬프와 함께 LyricSegment로 반환합니다."""
        print(f"🎤 오디오 전사 중: {Path(audio_path).name}")
        
        try:
            segments, info = self.model.transcribe(
                audio_path,
                beam_size=5,
                word_timestamps=True,
                language=language
            )
            
            segment_list = []
            for segment in segments:
                word_timings = []
                if hasattr(segment, 'words') and segment.words:
                    for word in segment.words:
                        word_data = {
                            'word': word.word.strip(),
                            'start': word.start,
                            'end': word.end,
                            'probability': getattr(word, 'probability', 0.0)
                        }
                        word_timings.append(word_data)
                
                lyric_segment = LyricSegment(
                    start_time=segment.start,
                    end_time=segment.end,
                    text=segment.text.strip(),
                    confidence=1.0,  # 전사된 텍스트는 높은 신뢰도
                    word_timings=word_timings if word_timings else None
                )
                segment_list.append(lyric_segment)
            
            print(f"✅ 전사 완료: {len(segment_list)} 세그먼트")
            print(f"🌍 감지된 언어: {info.language} (확률: {info.language_probability:.2f})")
            
            return segment_list
            
        except Exception as e:
            print(f"❌ 전사 실패: {e}")
            raise
    
    def align_lyrics_with_audio(self, audio_path: str, lyrics_path: str, 
                               language: Optional[str] = None,
                               similarity_threshold: float = 0.6) -> List[LyricSegment]:
        """기존 가사를 오디오와 동기화합니다."""
        print("🎵 가사 동기화 시작...")
        
        # 1단계: 가사 로드
        print("📖 가사 로딩 중...")
        lyrics_lines = self.lyrics_processor.load_lyrics_file(lyrics_path)
        print(f"✅ {len(lyrics_lines)}줄의 가사 로드 완료")
        
        # 2단계: 오디오 전사
        print("🎤 오디오 전사 중...")
        transcription_segments = self._transcribe_for_alignment(audio_path, language)
        
        # 3단계: 가사와 전사 텍스트 정렬
        print("🔄 가사와 오디오 정렬 중...")
        aligned_segments = self._align_text_with_timestamps(
            lyrics_lines, transcription_segments, similarity_threshold
        )
        
        print(f"✅ 동기화 완료: {len(aligned_segments)} 세그먼트 정렬됨")
        return aligned_segments
    
    def _transcribe_for_alignment(self, audio_path: str, language: Optional[str] = None) -> List[Dict]:
        """정렬을 위한 오디오 전사 (딕셔너리 형태로 반환)"""
        segments, info = self.model.transcribe(
            audio_path,
            beam_size=5,
            word_timestamps=True,
            language=language
        )
        
        segment_list = []
        for segment in segments:
            segment_data = {
                'start': segment.start,
                'end': segment.end,
                'text': segment.text.strip(),
                'words': []
            }
            
            if hasattr(segment, 'words') and segment.words:
                for word in segment.words:
                    word_data = {
                        'word': word.word.strip(),
                        'start': word.start,
                        'end': word.end,
                        'probability': getattr(word, 'probability', 0.0)
                    }
                    segment_data['words'].append(word_data)
            
            segment_list.append(segment_data)
        
        return segment_list
    
    def _align_text_with_timestamps(self, lyrics_lines: List[str], 
                                   transcription_segments: List[Dict],
                                   similarity_threshold: float) -> List[LyricSegment]:
        """가사 텍스트를 전사 타임스탬프와 정렬합니다."""
        aligned_segments = []
        normalized_lyrics = [
            self.lyrics_processor.normalize_text(line) for line in lyrics_lines
        ]
        
        used_segments = set()
        
        for i, (original_lyric, normalized_lyric) in enumerate(zip(lyrics_lines, normalized_lyrics)):
            best_match = None
            best_score = 0
            best_segment_indices = []
            
            # 최적 매칭 세그먼트 찾기
            for j, segment in enumerate(transcription_segments):
                if j in used_segments:
                    continue
                    
                normalized_segment = self.lyrics_processor.normalize_text(segment['text'])
                
                # 유사도 계산
                similarity_ratio = fuzz.ratio(normalized_lyric, normalized_segment) / 100.0
                partial_ratio = fuzz.partial_ratio(normalized_lyric, normalized_segment) / 100.0
                token_ratio = fuzz.token_sort_ratio(normalized_lyric, normalized_segment) / 100.0
                
                score = max(similarity_ratio, partial_ratio, token_ratio)
                
                if score > best_score and score >= similarity_threshold:
                    best_score = score
                    best_match = segment
                    best_segment_indices = [j]
            
            # 단일 매치가 없으면 연속 세그먼트 조합 시도
            if best_score < similarity_threshold:
                best_match, best_score, best_segment_indices = self._find_multi_segment_match(
                    normalized_lyric, transcription_segments, used_segments, similarity_threshold
                )
            
            # 정렬된 세그먼트 생성
            if best_match and best_score >= similarity_threshold:
                for idx in best_segment_indices:
                    used_segments.add(idx)
                
                if isinstance(best_match, list):
                    start_time = min(seg['start'] for seg in best_match)
                    end_time = max(seg['end'] for seg in best_match)
                    combined_words = []
                    for seg in best_match:
                        combined_words.extend(seg.get('words', []))
                else:
                    start_time = best_match['start']
                    end_time = best_match['end']
                    combined_words = best_match.get('words', [])
                
                aligned_segment = LyricSegment(
                    start_time=start_time,
                    end_time=end_time,
                    text=original_lyric,
                    confidence=best_score,
                    word_timings=combined_words
                )
                aligned_segments.append(aligned_segment)
                
                print(f"✓ 매칭됨: '{original_lyric[:50]}...' (신뢰도: {best_score:.2f})")
            else:
                print(f"⚠️ 매칭 실패: '{original_lyric[:50]}...'")
                # 추정 타이밍으로 세그먼트 추가
                if aligned_segments:
                    estimated_start = aligned_segments[-1].end_time + 0.5
                    estimated_end = estimated_start + 3.0
                else:
                    estimated_start = 0.0
                    estimated_end = 3.0
                
                aligned_segment = LyricSegment(
                    start_time=estimated_start,
                    end_time=estimated_end,
                    text=original_lyric,
                    confidence=0.0
                )
                aligned_segments.append(aligned_segment)
        
        return aligned_segments
    
    def _find_multi_segment_match(self, target_lyric: str, transcription_segments: List[Dict],
                                 used_segments: set, similarity_threshold: float) -> Tuple[Optional[List[Dict]], float, List[int]]:
        """여러 연속 전사 세그먼트와 가사 라인을 매칭 시도합니다."""
        best_match = None
        best_score = 0
        best_indices = []
        
        for window_size in range(2, 5):
            for start_idx in range(len(transcription_segments) - window_size + 1):
                if any(start_idx + i in used_segments for i in range(window_size)):
                    continue
                
                combined_segments = transcription_segments[start_idx:start_idx + window_size]
                combined_text = " ".join([
                    self.lyrics_processor.normalize_text(seg['text']) 
                    for seg in combined_segments
                ])
                
                similarity_ratio = fuzz.ratio(target_lyric, combined_text) / 100.0
                token_ratio = fuzz.token_sort_ratio(target_lyric, combined_text) / 100.0
                score = max(similarity_ratio, token_ratio)
                
                if score > best_score and score >= similarity_threshold:
                    best_score = score
                    best_match = combined_segments
                    best_indices = list(range(start_idx, start_idx + window_size))
        
        return best_match, best_score, best_indices
    parser = argparse.ArgumentParser(description="faster-whisper를 사용하여 오디오 파일에서 텍스트를 추출합니다.")
    parser.add_argument("audio_file", type=str, help="텍스트를 추출할 오디오 파일 경로")
    parser.add_argument("--model_size", type=str, default="large-v3", help="사용할 Whisper 모델 크기 (예: tiny, base, small, medium, large-v3)")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cpu", "cuda"], help="사용할 장치 (auto, cpu, cuda)")
    parser.add_argument("--compute_type", type=str, default="default", choices=["default", "int8", "float16"], help="연산 타입 (메모리 및 속도 최적화)")
    parser.add_argument("--language", type=str, default=None, help="오디오의 언어 코드 (예: ko, en). 지정 시 언어 감지 과정을 생략하여 더 빠름.")
    
def transcribe_and_sync():
    """통합된 전사 및 동기화 함수"""
    parser = argparse.ArgumentParser(description="faster-whisper를 사용하여 오디오에서 텍스트를 추출하고 가사와 동기화합니다.")
    parser.add_argument("audio_file", type=str, help="처리할 오디오 파일 경로")
    parser.add_argument("--lyrics_file", type=str, default=None, help="동기화할 가사 파일 경로 (선택사항)")
    parser.add_argument("--model_size", type=str, default="large-v3", help="사용할 Whisper 모델 크기")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cpu", "cuda"], help="사용할 장치")
    parser.add_argument("--compute_type", type=str, default="default", choices=["default", "int8", "float16"], help="연산 타입")
    parser.add_argument("--language", type=str, default=None, help="오디오의 언어 코드")
    parser.add_argument("--similarity_threshold", type=float, default=0.6, help="텍스트 매칭을 위한 최소 유사도")
    parser.add_argument("--output_dir", type=str, default=None, help="출력 디렉토리 (기본값: 오디오 파일과 같은 위치)")
    
    args = parser.parse_args()

    audio_path = Path(args.audio_file)
    if not audio_path.exists():
        print(f"❌ 오류: 파일을 찾을 수 없습니다 -> {audio_path}")
        return

    # 출력 디렉토리 설정
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = audio_path.parent
    
    output_dir.mkdir(exist_ok=True)
    output_basename = output_dir / audio_path.stem

    print("="*60)
    if args.lyrics_file:
        print(f"🎵 가사 동기화 모드: {audio_path.name}")
        print(f"가사 파일: {args.lyrics_file}")
    else:
        print(f"🎤 오디오 전사 모드: {audio_path.name}")
    print(f"모델: {args.model_size}, 장치: {args.device}, 연산 타입: {args.compute_type}")
    print("="*60)

    try:
        # 전사기 초기화
        transcriber = AudioTranscriber(
            model_size=args.model_size, 
            device=args.device
        )
        
        start_time = time.perf_counter()
        
        if args.lyrics_file:
            # 가사 동기화 모드
            if not Path(args.lyrics_file).exists():
                print(f"❌ 가사 파일을 찾을 수 없습니다: {args.lyrics_file}")
                return
            
            segments = transcriber.align_lyrics_with_audio(
                str(audio_path), 
                args.lyrics_file,
                language=args.language,
                similarity_threshold=args.similarity_threshold
            )
            
            # 동기화 결과 요약
            print_sync_summary(segments)
            
        else:
            # 순수 전사 모드
            segments = transcriber.transcribe_with_timestamps(
                str(audio_path), 
                language=args.language
            )
        
        end_time = time.perf_counter()
        
        print(f"\n⏱️ 처리 완료! (소요 시간: {end_time - start_time:.2f}초)")
        
        # 다양한 형식으로 결과 저장
        save_as_txt(segments, output_basename.with_suffix(".txt"))
        save_as_srt(segments, output_basename.with_suffix(".srt"))
        save_as_lrc(segments, output_basename.with_suffix(".lrc"))
        save_as_json(segments, output_basename.with_suffix(".json"))
        
        print(f"\n🎉 모든 파일이 저장되었습니다: {output_dir}")

    except Exception as e:
        print(f"❌ 오류 발생: {e}")
        raise

def print_sync_summary(segments: List[LyricSegment]):
    """동기화 결과 요약을 출력합니다."""
    total_segments = len(segments)
    high_confidence = sum(1 for seg in segments if seg.confidence >= 0.8)
    medium_confidence = sum(1 for seg in segments if 0.5 <= seg.confidence < 0.8)
    low_confidence = sum(1 for seg in segments if seg.confidence < 0.5)
    
    print(f"\n📊 동기화 결과 요약:")
    print(f"   전체 세그먼트: {total_segments}")
    print(f"   높은 신뢰도 (≥80%): {high_confidence}")
    print(f"   중간 신뢰도 (50-79%): {medium_confidence}")
    print(f"   낮은 신뢰도 (<50%): {low_confidence}")
    
    if segments:
        total_duration = segments[-1].end_time - segments[0].start_time
        print(f"   전체 길이: {total_duration:.1f}초")
        avg_confidence = sum(seg.confidence for seg in segments) / len(segments)
        print(f"   평균 신뢰도: {avg_confidence:.2f}")

if __name__ == "__main__":
    transcribe_and_sync()